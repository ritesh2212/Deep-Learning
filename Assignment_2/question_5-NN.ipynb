{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](kaggle_result.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOP 3 Submission:\n",
    "### 0.15122 (Architecture : 2000 Neurons, 200 Epochs, learning rate : 0.001 ) : \n",
    "This model model converged best with the above setup. As learning rate is sufficient for model to learn also sufficient neurons to fire.\n",
    "### 0.15244 (Architecture : 2000 Neurons, 250 Epochs, learning rate : 0.001 ) :\n",
    "This model maybe gets little more converged than the above one as it has same architecture but it is running for 250 epochs \n",
    "### 0.19819 (Architecture : 1500 Neurons, 150 Epochs, learning rate : 0.001 ) :\n",
    "This model has less neurons and also less epochs so less neurons are firing and also the model is not fully converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn import preprocessing \n",
    "import torch.utils.data as utils\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Files and droping ID column from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('./dataset/kaggle/train.csv',sep=',')\n",
    "\n",
    "test_dataset = pd.read_csv('./dataset/kaggle/test.csv',sep=',')\n",
    "test_dataset = test_dataset.drop('Id',axis=1)\n",
    "label = train_dataset.SalePrice\n",
    "train_dataset = train_dataset.drop(['SalePrice','Id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC Fence  \\\n",
       "0         Lvl    AllPub    Inside  ...           0        0    NaN   NaN   \n",
       "1         Lvl    AllPub       FR2  ...           0        0    NaN   NaN   \n",
       "2         Lvl    AllPub    Inside  ...           0        0    NaN   NaN   \n",
       "3         Lvl    AllPub    Corner  ...           0        0    NaN   NaN   \n",
       "4         Lvl    AllPub       FR2  ...           0        0    NaN   NaN   \n",
       "\n",
       "  MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition  \n",
       "0         NaN       0       2    2008        WD         Normal  \n",
       "1         NaN       0       5    2007        WD         Normal  \n",
       "2         NaN       0       9    2008        WD         Normal  \n",
       "3         NaN       0       2    2006        WD        Abnorml  \n",
       "4         NaN       0      12    2008        WD         Normal  \n",
       "\n",
       "[5 rows x 79 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Categorical value with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeNanCategorical(dataset):\n",
    "    categorical = ['Alley','FireplaceQu','Fence','MiscFeature','PoolQC','GarageQual','GarageFinish','GarageType','GarageCond','BsmtQual','BsmtCond','BsmtFinType1','BsmtFinType2','BsmtExposure']\n",
    "    for col in categorical:\n",
    "        dataset[col].fillna('NA', inplace=True)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Numerical value with the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeNanNumerical(dataset,y):\n",
    "    for i in range(y):\n",
    "        #print(test_dataset.isna().sum().sort_values()[-18:-1].to_frame().index[i])\n",
    "        col = dataset.isna().sum().sort_values()[-(y+2):-1].to_frame().index[i]\n",
    "        dataset[col].fillna(dataset[col].value_counts().to_frame().index[0],inplace=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestData replacing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = changeNanCategorical(test_dataset)\n",
    "test_dataset = changeNanNumerical(test_dataset,78)\n",
    "test_dataset.LotFrontage.fillna(test_dataset.LotFrontage.value_counts().to_frame().index[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainData replacing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = changeNanCategorical(train_dataset)\n",
    "train_dataset = changeNanNumerical(train_dataset,78)\n",
    "train_dataset.LotFrontage.fillna(train_dataset.LotFrontage.value_counts().to_frame().index[0],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get categorical and numerical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCategoricalColumn(dataset):\n",
    "    column = []\n",
    "    for col in dataset.columns:\n",
    "        if isinstance(dataset[col].unique()[0], str):\n",
    "            column.append(col)\n",
    "    return column\n",
    "categoricalColum = getCategoricalColumn(train_dataset)\n",
    "num_cols = train_dataset._get_numeric_data().columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dummy dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = pd.DataFrame()\n",
    "X_train_Cat = pd.DataFrame()\n",
    "X_test_num = pd.DataFrame()\n",
    "X_test_Cat = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Numerical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in num_cols:\n",
    "    X_train_num[i] = train_dataset[i]\n",
    "    X_test_num[i] = test_dataset[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalise the Numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = (X_train_num-X_train_num.mean())/X_train_num.std()\n",
    "X_test_num = (X_test_num-X_test_num.mean())/X_test_num.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Categorical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categoricalColum:\n",
    "    X_train_Cat[i] = train_dataset[i]\n",
    "    X_test_Cat[i] = test_dataset[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding on Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeStrColInOneHotEncoding(dataset):\n",
    "    for col in dataset.columns:\n",
    "        if isinstance(dataset[col].unique()[0], str):\n",
    "            # Get one hot encoding of columns B\n",
    "            one_hot = pd.get_dummies(dataset[col],prefix=str(col+\"_\"))\n",
    "            # Drop column B as it is now encoded\n",
    "            dataset = dataset.drop(col,axis = 1)\n",
    "            # Join the encoded df\n",
    "            dataset = dataset.join(one_hot)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "X_train_Cat = changeStrColInOneHotEncoding(X_train_Cat)\n",
    "X_test_Cat = changeStrColInOneHotEncoding(X_test_Cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat Categorical and numerical column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_Cat, X_train_num], axis=1)\n",
    "X_test = pd.concat([X_test_Cat, X_test_num], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add column in Test data which are not present from Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  X_train.columns:\n",
    "    if i not in X_test.columns:\n",
    "        ind = X_train.columns.get_loc(i)\n",
    "        X_test.insert(ind,i,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isna().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 302)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data in torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_tensor = torch.tensor(X_train)\n",
    "torch_tensor = torch_tensor.type(torch.FloatTensor)\n",
    "label = label.values.astype(np.float)\n",
    "label_tensor = torch.tensor(label)\n",
    "label_tensor =  label_tensor.type(torch.FloatTensor)\n",
    "\n",
    "tensor_x,tensor_y = torch_tensor,label_tensor\n",
    "my_dataset = utils.TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "train_loader = utils.DataLoader(my_dataset,batch_size=1) # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a FC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=302, out_features=2500, bias=True)\n",
      "  (fc5): Linear(in_features=2500, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(size, 2500)  \n",
    "        #self.fc2 = nn.Linear(1024, 256)\n",
    "        #self.fc3 = nn.Linear(256, 128)\n",
    "        #self.fc4 = nn.Linear(128, 16)\n",
    "        self.fc5 = nn.Linear(2500,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        #x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Net(tensor_x.shape[1])\n",
    "model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize criterion and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  0  loss :  19936008237.978073\n",
      "Iter  1  loss :  3096127447.4531803\n",
      "Iter  2  loss :  1974120493.7309115\n",
      "Iter  3  loss :  1589802617.1549587\n",
      "Iter  4  loss :  1389894772.5745742\n",
      "Iter  5  loss :  1259415765.2417643\n",
      "Iter  6  loss :  1168705438.8682396\n",
      "Iter  7  loss :  1103164201.9076674\n",
      "Iter  8  loss :  1054047235.1013278\n",
      "Iter  9  loss :  1015957556.9525133\n",
      "Iter  10  loss :  985473204.5102208\n",
      "Iter  11  loss :  960395090.0405735\n",
      "Iter  12  loss :  939251864.8032942\n",
      "Iter  13  loss :  921046592.986016\n",
      "Iter  14  loss :  905101207.5306957\n",
      "Iter  15  loss :  890936516.7469901\n",
      "Iter  16  loss :  878222944.9893478\n",
      "Iter  17  loss :  866708703.2952877\n",
      "Iter  18  loss :  856202500.1868601\n",
      "Iter  19  loss :  846550502.2017279\n",
      "Iter  20  loss :  837634214.839999\n",
      "Iter  21  loss :  829357069.1709275\n",
      "Iter  22  loss :  821640221.2131428\n",
      "Iter  23  loss :  814415028.0842257\n",
      "Iter  24  loss :  807627438.1822406\n",
      "Iter  25  loss :  801227199.807112\n",
      "Iter  26  loss :  795173220.6412601\n",
      "Iter  27  loss :  789426264.9853783\n",
      "Iter  28  loss :  783958487.2444973\n",
      "Iter  29  loss :  778744118.9014558\n",
      "Iter  30  loss :  773766432.5326704\n",
      "Iter  31  loss :  769012083.3081961\n",
      "Iter  32  loss :  764458433.2462889\n",
      "Iter  33  loss :  760088362.3654501\n",
      "Iter  34  loss :  755882976.4989038\n",
      "Iter  35  loss :  751828745.556323\n",
      "Iter  36  loss :  747920104.859467\n",
      "Iter  37  loss :  744145857.5789422\n",
      "Iter  38  loss :  740498410.3968414\n",
      "Iter  39  loss :  736967765.6858928\n",
      "Iter  40  loss :  733547363.3257277\n",
      "Iter  41  loss :  730224049.0619541\n",
      "Iter  42  loss :  726992587.182092\n",
      "Iter  43  loss :  723847749.5154316\n",
      "Iter  44  loss :  720792175.9438301\n",
      "Iter  45  loss :  717822214.6945019\n",
      "Iter  46  loss :  714932267.9187444\n",
      "Iter  47  loss :  712113545.1022326\n",
      "Iter  48  loss :  709366215.2491862\n",
      "Iter  49  loss :  706681344.5822867\n",
      "Iter  50  loss :  704057657.4360206\n",
      "Iter  51  loss :  701493783.0189171\n",
      "Iter  52  loss :  698982886.847109\n",
      "Iter  53  loss :  696520873.2226914\n",
      "Iter  54  loss :  694104959.1887928\n",
      "Iter  55  loss :  691736213.0524837\n",
      "Iter  56  loss :  689412926.6296698\n",
      "Iter  57  loss :  687127553.1636668\n",
      "Iter  58  loss :  684878734.4959372\n",
      "Iter  59  loss :  682666484.9844822\n",
      "Iter  60  loss :  680490170.2619721\n",
      "Iter  61  loss :  678357143.682925\n",
      "Iter  62  loss :  676255345.8156567\n",
      "Iter  63  loss :  674193347.741679\n",
      "Iter  64  loss :  672164962.9431576\n",
      "Iter  65  loss :  670169650.4091855\n",
      "Iter  66  loss :  668206166.3138849\n",
      "Iter  67  loss :  666273889.5468783\n",
      "Iter  68  loss :  664368189.1583817\n",
      "Iter  69  loss :  662494030.6497407\n",
      "Iter  70  loss :  660645732.6514075\n",
      "Iter  71  loss :  658820267.8751217\n",
      "Iter  72  loss :  657021301.0780582\n",
      "Iter  73  loss :  655246427.9408232\n",
      "Iter  74  loss :  653491136.6973422\n",
      "Iter  75  loss :  651763299.8204243\n",
      "Iter  76  loss :  650054468.6949906\n",
      "Iter  77  loss :  648370377.3077995\n",
      "Iter  78  loss :  646707198.1632926\n",
      "Iter  79  loss :  645067523.0023916\n",
      "Iter  80  loss :  643446250.8658547\n",
      "Iter  81  loss :  641843511.5748963\n",
      "Iter  82  loss :  640258585.1653832\n",
      "Iter  83  loss :  638690648.077963\n",
      "Iter  84  loss :  637137223.4111105\n",
      "Iter  85  loss :  635605189.5461298\n",
      "Iter  86  loss :  634090354.0423998\n",
      "Iter  87  loss :  632590475.8970636\n",
      "Iter  88  loss :  631107348.8284645\n",
      "Iter  89  loss :  629642809.6805108\n",
      "Iter  90  loss :  628192931.0541358\n",
      "Iter  91  loss :  626760621.3915305\n",
      "Iter  92  loss :  625339608.6650932\n",
      "Iter  93  loss :  623936172.3737895\n",
      "Iter  94  loss :  622543026.9310346\n",
      "Iter  95  loss :  621167337.4984405\n",
      "Iter  96  loss :  619802215.007578\n",
      "Iter  97  loss :  618451390.684089\n",
      "Iter  98  loss :  617114790.1387314\n",
      "Iter  99  loss :  615790039.3352122\n",
      "Iter  100  loss :  614476090.8442523\n",
      "Iter  101  loss :  613175138.0085905\n",
      "Iter  102  loss :  611884414.5096502\n",
      "Iter  103  loss :  610605692.740161\n",
      "Iter  104  loss :  609336678.3017378\n",
      "Iter  105  loss :  608082305.8190649\n",
      "Iter  106  loss :  606834850.730549\n",
      "Iter  107  loss :  605598025.1869861\n",
      "Iter  108  loss :  604369500.8055522\n",
      "Iter  109  loss :  603155538.4372733\n",
      "Iter  110  loss :  601945953.3085955\n",
      "Iter  111  loss :  600747402.6408063\n",
      "Iter  112  loss :  599559518.593791\n",
      "Iter  113  loss :  598380932.2031657\n",
      "Iter  114  loss :  597209168.562194\n",
      "Iter  115  loss :  596046862.0031596\n",
      "Iter  116  loss :  594891494.9381033\n",
      "Iter  117  loss :  593743472.8857104\n",
      "Iter  118  loss :  592605647.5427957\n",
      "Iter  119  loss :  591474095.8226482\n",
      "Iter  120  loss :  590350151.052717\n",
      "Iter  121  loss :  589234397.4325491\n",
      "Iter  122  loss :  588125895.4520909\n",
      "Iter  123  loss :  587023899.7252672\n",
      "Iter  124  loss :  585929068.4319218\n",
      "Iter  125  loss :  584839783.4895878\n",
      "Iter  126  loss :  583759042.036969\n",
      "Iter  127  loss :  582685080.8431935\n",
      "Iter  128  loss :  581611754.7682649\n",
      "Iter  129  loss :  580550033.3356758\n",
      "Iter  130  loss :  579491215.2713394\n",
      "Iter  131  loss :  578439395.236562\n",
      "Iter  132  loss :  577395283.1838971\n",
      "Iter  133  loss :  576357057.6155158\n",
      "Iter  134  loss :  575318644.0044078\n",
      "Iter  135  loss :  574293885.9756684\n",
      "Iter  136  loss :  573270720.4784985\n",
      "Iter  137  loss :  572252333.2248669\n",
      "Iter  138  loss :  571241817.558458\n",
      "Iter  139  loss :  570236836.4387904\n",
      "Iter  140  loss :  569236125.8591263\n",
      "Iter  141  loss :  568241758.6647847\n",
      "Iter  142  loss :  567252236.7794558\n",
      "Iter  143  loss :  566267086.0083419\n",
      "Iter  144  loss :  565285502.2011977\n",
      "Iter  145  loss :  564313629.4163362\n",
      "Iter  146  loss :  563342858.8477845\n",
      "Iter  147  loss :  562374923.6731948\n",
      "Iter  148  loss :  561411507.7892756\n",
      "Iter  149  loss :  560455691.748619\n",
      "Iter  150  loss :  559503817.6000698\n",
      "Iter  151  loss :  558558729.1064991\n",
      "Iter  152  loss :  557611222.7320516\n",
      "Iter  153  loss :  556672420.2314085\n",
      "Iter  154  loss :  555736971.9087293\n",
      "Iter  155  loss :  554806634.1763192\n",
      "Iter  156  loss :  553878742.3588773\n",
      "Iter  157  loss :  552955024.112596\n",
      "Iter  158  loss :  552035913.48998\n",
      "Iter  159  loss :  551118948.1944543\n",
      "Iter  160  loss :  550211134.5236739\n",
      "Iter  161  loss :  549299155.9843948\n",
      "Iter  162  loss :  548397337.376724\n",
      "Iter  163  loss :  547498138.6494721\n",
      "Iter  164  loss :  546597416.3754601\n",
      "Iter  165  loss :  545703220.1349486\n",
      "Iter  166  loss :  544811118.100305\n",
      "Iter  167  loss :  543927934.9543508\n",
      "Iter  168  loss :  543042559.9621716\n",
      "Iter  169  loss :  542158658.4247282\n",
      "Iter  170  loss :  541285528.266633\n",
      "Iter  171  loss :  540408055.7852886\n",
      "Iter  172  loss :  539537706.0610306\n",
      "Iter  173  loss :  538663757.0746518\n",
      "Iter  174  loss :  537790866.9296128\n",
      "Iter  175  loss :  536936994.1747346\n",
      "Iter  176  loss :  536074214.6999032\n",
      "Iter  177  loss :  535218674.75665635\n",
      "Iter  178  loss :  534362703.13965696\n",
      "Iter  179  loss :  533511474.9690186\n",
      "Iter  180  loss :  532660652.77504516\n",
      "Iter  181  loss :  531816507.80660236\n",
      "Iter  182  loss :  530969107.5363425\n",
      "Iter  183  loss :  530129014.1595218\n",
      "Iter  184  loss :  529291776.28348607\n",
      "Iter  185  loss :  528458531.26623034\n",
      "Iter  186  loss :  527623886.3065331\n",
      "Iter  187  loss :  526795556.03127575\n",
      "Iter  188  loss :  525965131.41914916\n",
      "Iter  189  loss :  525141848.32878184\n",
      "Iter  190  loss :  524322734.61444443\n",
      "Iter  191  loss :  523502168.0373162\n",
      "Iter  192  loss :  522684033.2619788\n",
      "Iter  193  loss :  521873077.28318423\n",
      "Iter  194  loss :  521063187.2661223\n",
      "Iter  195  loss :  520252759.9271135\n",
      "Iter  196  loss :  519445369.36552954\n",
      "Iter  197  loss :  518645336.4179848\n",
      "Iter  198  loss :  517844405.31714267\n",
      "Iter  199  loss :  517041174.730956\n",
      "Iter  200  loss :  516246224.3263242\n",
      "Iter  201  loss :  515449833.59468895\n",
      "Iter  202  loss :  514661882.06538606\n",
      "Iter  203  loss :  513866977.41113716\n",
      "Iter  204  loss :  513079544.79968745\n",
      "Iter  205  loss :  512292667.41184014\n",
      "Iter  206  loss :  511509829.45646673\n",
      "Iter  207  loss :  510726924.49144137\n",
      "Iter  208  loss :  509942588.5769297\n",
      "Iter  209  loss :  509166069.2335355\n",
      "Iter  210  loss :  508384115.8463486\n",
      "Iter  211  loss :  507613435.1827864\n",
      "Iter  212  loss :  506840297.5881699\n",
      "Iter  213  loss :  506065912.3010886\n",
      "Iter  214  loss :  505295107.4469327\n",
      "Iter  215  loss :  504525830.5549217\n",
      "Iter  216  loss :  503762190.7413845\n",
      "Iter  217  loss :  502994665.016782\n",
      "Iter  218  loss :  502230156.34099394\n",
      "Iter  219  loss :  501470060.53756356\n",
      "Iter  220  loss :  500704542.7883058\n",
      "Iter  221  loss :  499942957.3612583\n",
      "Iter  222  loss :  499181074.3928385\n",
      "Iter  223  loss :  498422377.0100517\n",
      "Iter  224  loss :  497658122.28968877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter  225  loss :  496901679.3773233\n",
      "Iter  226  loss :  496140831.78981483\n",
      "Iter  227  loss :  495379276.22507375\n",
      "Iter  228  loss :  494619378.4544646\n",
      "Iter  229  loss :  493862930.29593015\n",
      "Iter  230  loss :  493097044.4413213\n",
      "Iter  231  loss :  492340981.70412195\n",
      "Iter  232  loss :  491582621.4101811\n",
      "Iter  233  loss :  490828641.2774133\n",
      "Iter  234  loss :  490067220.0746915\n",
      "Iter  235  loss :  489308762.0377131\n",
      "Iter  236  loss :  488548510.0053115\n",
      "Iter  237  loss :  487796427.9520482\n",
      "Iter  238  loss :  487028741.34611434\n",
      "Iter  239  loss :  486269490.3603611\n",
      "Iter  240  loss :  485508052.161582\n",
      "Iter  241  loss :  484742547.3467497\n",
      "Iter  242  loss :  483970007.0017275\n",
      "Iter  243  loss :  483212386.72367364\n",
      "Iter  244  loss :  482440151.53529155\n",
      "Iter  245  loss :  481671293.1399789\n",
      "Iter  246  loss :  480903486.2507883\n",
      "Iter  247  loss :  480150855.05673814\n",
      "Iter  248  loss :  479361014.43455374\n",
      "Iter  249  loss :  478597854.83006394\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(250):\n",
    "    ave_loss=0.0\n",
    "    \n",
    "    for i,data in enumerate(train_loader,0):\n",
    "        inputs,target =data\n",
    "        inputs = inputs.float().cuda()\n",
    "        target = target.float().cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output=model(inputs)\n",
    "        output = output.squeeze(1)\n",
    "        loss=criterion(output,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ave_loss+=loss.item()\n",
    "    print(\"Iter \",epoch,\" loss : \",ave_loss/train_dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1459, 302])\n"
     ]
    }
   ],
   "source": [
    "torch_tensor = torch.Tensor(X_test)\n",
    "x_test = torch_tensor.type(torch.FloatTensor)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = model(x_test.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[108431.2500],\n",
       "        [149578.7656],\n",
       "        [182792.8906],\n",
       "        ...,\n",
       "        [162402.4531],\n",
       "        [109577.7969],\n",
       "        [216829.1562]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test = predict_test.cpu()\n",
    "predict_test = predict_test.detach().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "636245.3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=['Id', 'SalePrice'])\n",
    "\n",
    "test_dataset1 = pd.read_csv('./dataset/kaggle/test.csv',sep=',')\n",
    "test_df['Id'] = test_dataset1.Id\n",
    "\n",
    "test_df['SalePrice'] = predict_test\n",
    "\n",
    "test_df.to_csv(\"submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
